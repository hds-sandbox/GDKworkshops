---
title: "Introduction to workflows with gwf"
subtitle: https://hds-sandbox.github.io/GDKworkshops
author: 
    - "Samuele Soraggi"
    - "Dan Søndergaard"
institute:
    - Health Data Science sandbox, BiRC
    - GenomeDK, Health
date: 6.March 2024
format: revealjs
scrollable: true
toc: true
toc-depth: 1
slide-number: h.v
code-line-numbers: false
logo: img/logos.png
navigation-mode: vertical

---

# Short on Workflows and gwf

## Workflow

![](img/small_pipeline.png)

:::{.callout-note title="Workflow and W. Management System"}
A **workflow** is a series of calculations and data manipulations which have to be performed in a specific sequence. 
A **workflow management system** organizes the workflow steps through defined dependencies, can assign different computing resources to each step, 
keeps a log of the workflow, interacts with a cluster's queueing system. 
:::

## A few terms

![](img/small_pipeline_terms.png)

Each target of the workflow is **submitted** to a queueing manager so that it enters the queue and can be executed when possible.

## Gwf

A lightweight and easy to adopt workflow manager. it requires knowing or larning Python, but the time invested in it **pays back quickly**. Some useful features:

- Defines **target dependencies automatically** using the input and output files of targets
- Only submits targets when their output files are **not up to date**
- Fire-and-forget: once you submit targets, you need **not do more**
- Has a few, simple and useful commands for **following workflow execution** and **cleaning up** intermediate data files

Check out [the gwf webpage](https://gwf.app/) for updates, examples and more detailed documentation.

# Exercise setup

Data and inspiration from [this SW carpentry bioinformatics tutorial](https://datacarpentry.org/wrangling-genomics/index.html), which does not use pipelines.

WHOLE WORKFLOW FIGURE

## Download examples and data

Examples and reference file

```{.bash}
curl -L -o gwf_workshop.zip https://github.com/hds-sandbox/GDKworkshops/archive/main.zip \
    && unzip gwf_workshop.zip \
    && mv GDKworkshops-main/Examples/gwf gwf_workshop \
    && rm -rf GDKworkshops-main gwf_workshop.zip
```


Download fastq files

```{.bash}
mkdir -p gwf_workshop/data
cd gwf_workshop/data/

curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/004/SRR2589044/SRR2589044_1.fastq.gz
curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/004/SRR2589044/SRR2589044_2.fastq.gz
curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/003/SRR2584863/SRR2584863_1.fastq.gz
curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/003/SRR2584863/SRR2584863_2.fastq.gz
curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/006/SRR2584866/SRR2584866_1.fastq.gz
curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/006/SRR2584866/SRR2584866_2.fastq.gz

cd ..
```

---

Final file structure (`tree` command)

![](img/tree_download.png)

## Install gwf

Using conda to create a new environment from the `yml` file:

```{.bash}
conda create -n gwf_workshop -f Environments/gwf_workshop.yml
conda activate gwf_workshop
```

You can see the packages installed with

```{.bash}
cat Environments/gwf_workshop.yml
```

# A single target Workflow

FIGURE

The workflow file can be printed out with 

```{.bash}
less -S WF01_report.py
```

---

You can easily spot all the elements of the workflow graph:

```{.python code-line-numbers="1-3|6|7-9|10-12|13"}
from gwf import Workflow

gwf = Workflow(defaults={'account': 'your_project'})

## A target doing FastQC on a file
gwf.target('FastQC', 
           cores=1,
           memory='8gb',
           walltime='00:05:00',	
           inputs=['data/SRR2584863_1.fastq.gz'], 
           outputs=['data/SRR2584863_1_fastqc.html', 
                    'data/SRR2584863_1_fastqc.zip']) << """ 
fastqc data/SRR2584863_1.fastq.gz
"""
```

## Status and submission

We can see what is the status of each target - if it doesn't see the output files (`shouldrun`), if the output is there (`completed`), if it is executing a target (`running`), or if it failed ('failed').

```{.bash}
gwf -f WF01_report.py status
```

will show

```{.bash }
⨯ FastQC        shouldrun
```

. . .

You can submit the targets with 

```{.bash}
gwf -f WF01_report.py run
```

. . .

Look again at the status. You can follow it up in real-time using `watch` (it will lose the nice colour formatting):

```{.bash}
watch gwf -f WF01_report.py status
```

---

## Targets stdout and stderr

What are the commands in a target outputting? You can see `stdout` and `stderr` in the log of a target:

```{.bash}
gwf -f WF01_report.py logs FastQC
```

and

```{.bash}
gwf -f WF01_report.py logs -e FastQC
```

You can still use `watch` to follow in real time an output while a target is running

# Parallel target dependencies

FIGURE

---

You can use python to automatically parsing names in a folder and creating targets:

```{.python code-line-numbers="1-5|8|9-10|10-12|16-18|19-20"}
#find .fastq.gz files in the data folder
pattern = os.path.join('data', '*.fastq.gz')
fastq_files = glob.glob(pattern) 
#keep only the file name without the folder path
fastq_files = [os.path.basename(file) for file in fastq_files] #remove the path from the file


for FASTQ in fastq_files:
    #name without extensions
    FASTQ_BASENAME = FASTQ.split(".")[0] 
    #target with file-dependent name and inputs/outputs
    gwf.target( f'FastQC_{FASTQ_BASENAME}', 
           cores=1,
           memory='8gb',
           walltime='00:05:00',	
           inputs=[f'data/{FASTQ}'], 
           outputs=[f'data/{FASTQ_BASENAME}_fastqc.html', 
                    f'data/{FASTQ_BASENAME}_fastqc.zip']) << """ 
    fastqc data/{FASTQ}
    """.format(FASTQ=FASTQ)
```


## Useful gwf checks

You can see the status with specific messages about the status of each target and what causes it.

```
gwf -f WF02_report_parallel.py -v debug status
```

At the end of the debug messages, you will see that one FastQC is already complete from the first example.

```
⨯ FastQC_SRR2584866_1 shouldrun
⨯ FastQC_SRR2589044_1 shouldrun
✓ FastQC_SRR2584863_1 completed
⨯ FastQC_SRR2584863_2 shouldrun
⨯ FastQC_SRR2589044_2 shouldrun
⨯ FastQC_SRR2584866_2 shouldrun
```

. . .

You can print explicitely some targets of your choice and their dependencies. For example all the targets related to the first element of the fastq pairs:

```{.bash}
gwf -f WF02_report_parallel.py info FastQC*_1
```

Giving no names will print all the targets

---

Try to run the whole workflow and follow its status with `watch`.

```
gwf -f WF02_report_parallel.py run
```

and

```
watch gwf -f WF02_report_parallel.py status
```

&nbsp;

Targets will run independently and might run at the same time. 

# Multiple target dependencies

FIGURE

The MultQC target will have to wait for all the FastQC outputs

---

```{.python code-line-numbers="1-2|4|8-11|12-14"}
#names without path and extensions
fastq_basenames = [ FASTQ.split(".")[0] for FASTQ in fastq_files ] 

gwf.target( 'MultiQC', #name of the target
           cores=1,
           memory='8gb',
           walltime='00:05:00',	
           inputs= {'ZIP': [f'data/{FASTQ_BASENAME}_fastqc.zip' for FASTQ_BASENAME in fastq_basenames],
                    'REPORTS': [f'data/{FASTQ_BASENAME}_fastqc.html' for FASTQ_BASENAME in fastq_basenames] }, 
           outputs=['results/multiqc_output/multiqc_report.html'],
           protect=['results/multiqc_output/multiqc_report.html']) << """ 
    mkdir -p results/multiqc_output
    multiqc --outdir results/multiqc_output \
            data/
    """
```

## protected outputs and cleaning

The workflow parameter

```{.python}
protect=['results/multiqc_output/multiqc_report.html'])
```

prevents you from deleting the specified file when using `clean`. The `clean` command removes all **intermediate outputs** of a workflow (not essential for the result report and which **can be replicated** by rerunning the workflow). 
&nbsp;

:::{.callout-note title="Endpoints' files are protected"}
Files generated by `endpoints` (targets which are not dependencies of others, i.e. at the end of the workflow) do not get deleted. `clean --all` removes any file in the workflow.
:::

## status with filters

We know all FastQC targets are completed. Run first the workflow

```{.bash}
gwf -f WF03_multi_report.py run
```

. . .

&nbsp;

Now look only at the endpoints to avoid a long list of FastQC targets:

```{.bash}
gwf -f WF03_multi_report.py status --endpoints
```

gives

```{.bash}
✓ MultiQC       completed
```

. . .

Are you waiting for many endpoints? Look only for the `completed` ones:

```{.bash}
gwf -f WF03_multi_report.py status --endpoints -s completed
```

(You can also use `running`, `shouldrun` or `failed`).


# Exercise 1

FIGURE

You are going to trim the data with parallel targets

## Tasks-questions

Consider the workflow file `WF04_trimming.py`:

&nbsp;

1. apply `gwf status` to watch the **endpoints of the workflow**
2. Which is the file dependency **common to all endpoints**?
3. run the workflow and...
4. ...apply `watch gwf status` and follow only the `running` targets 
5. Look at the `stderr` printout of one trimming target

## Answers

**1. apply `gwf status` to watch the endpoints of the workflow**

&nbsp;

```{.bash}
gwf -f WF04_trimming.py status --endpoints
```

gives

```{.bash}
⨯ Trimmomatic_SRR2584863 shouldrun
⨯ Trimmomatic_SRR2584866 shouldrun
⨯ Trimmomatic_SRR2589044 shouldrun
```

---

**2. Which is the file dependency common to all endpoints?**

&nbsp;

Each endpoint needs an adapter `fasta` file apart from the raw data files in input. You can look at the file dependencies using

```{.bash}
gwf -f WF04_trimming.py info Trimmomatic*
```

or 

```{.bash}
gwf -f WF04_trimming.py info Trimmomatic* | less -S
```

for better formatting.

---

**3. run the workflow**
**4. use `watch gwf status` and follow only the `running` targets**

&nbsp;

After running the targets, for example with

```{.bash}
gwf -f WF04_trimming.py run Trimmomatic*
```

you can follow the running targets with

```{.bash}
watch gwf -f WF04_trimming.py status -s running
```

If the targets are currently `running` and neither `shouldrun`, `completed` nor `failed`, you should see something like this, 

```{.bash}
↻ Trimmomatic_SRR2584863 running
↻ Trimmomatic_SRR2584866 running
↻ Trimmomatic_SRR2589044 running
```

after which the targets will be `completed`.

---

**5. Look at the stderr printout of one trimming target**

&nbsp;

You can see the `stderr` f a trimming job, for example, by

```{.bash}
gwf -f WF04_trimming.py logs -e Trimmomatic_SRR2589044 | less -S
```

which should also make it easier to scroll and format when it is a long output

# Exercise 2

FIGURE

You will align the trimmed raw files, merge them and plot a histogram of each alignment's depth through an `R` script.

## Tasks-questions

Consider the workflow file `WF05_align_merge_plot.py`:

&nbsp;

1. use `status` to see which are the endpoint targets, then find their target dependencies
2. do you need to protect the final merged alignment in the target `BAM_merge`?
3. can the targets `BAM_merge` and `Histogram` run at the same time?
4. two raw fastq files are twice as larger than the others. If instead of aligning data in parallel targets you were to **align files one at a time in the same target**, how much more time would it take? (assuming parallel targets start running at the same time)
5. use `gwf clean --all` to remove all files from the old targets. Then run the workflow and `watch` the `status` of the targets. You should see the running targets follow the order of the targets dependencies.

## Answers

**1. use `status` to see which are the endpoint targets, then find their target dependencies**

&nbsp;

There are two endpoints: one for merging aligned files and one for plotting a histogram. Use

```{.bash}
gwf -f WF05_align_merge_plot.py status --endpoints
```
to obtain 
```{.bash}
⨯ BAM_merge     shouldrun
⨯ Histogram     shouldrun
```

. . .

Use the `info` command to see their file dependencies:

```{.bash}
gwf -f WF05_align_merge_plot.py info Histogram BAM_merge
```

Look at the output to find out that `Histogram` and `BAM_merge` depend both on the following targets

```{.bash}
"BWA_mem_SRR2589044",
"BWA_mem_SRR2584866",
"BWA_mem_SRR2584863"
```

. . .

Note though that they depend on different files from those targets!

---

**2. do you need to protect the final merged alignment in the target `BAM_merge`?**

&nbsp;

`BAM_merge` is an endpoint target, so its files do not need to be protected in case you want to do a cleanup with `gwf clean`. You can clean all files anyway with `gwf clean --all`. 

---

**3. can the targets `BAM_merge` and `Histogram` run at the same time?**

&nbsp;

Yes, as soon as the raw data files have been aligned separately.

---

**4. two raw fastq files are twice as larger than the others. If instead of aligning data in parallel targets you were to **align files one at a time in the same target**, how much more time would it take? (assuming parallel targets start running at the same time)**

&nbsp;

If you run a single target with the alignment of all `.fastq` files, it will take twice the time compared to running parallel targets. 

&nbsp;

. . .

When you run parallel targets, the time needed to finish the alignments is the time needed for the largest file (which is twice the size of the others).

FIGURE

&nbsp;

. . .

When you run sequentially the alignments, it takes the sum of all alignment times to get done.

FIGURE

# Clean up after yourself

FIGURE

---

Use the `tree` command to look at how many files you created!

FIGURE

---

What is the dize of each folder?

```{.bash}
du -h .
```

should give you this:

```{.bash}
aaaa
aaaa
```

---

Try to use 

```{.bash}
gwf -f WF05_align_merge_plot.py clean
```

accept the warning message and note how, already for a small exercise, you are going to cleanup a lot.

---

Now you have a much more slim folder, which contains workflows, raw data, a merged quality report, aligned data and depth histogram in `.pdf`.

FIGURE TREE

All the other files can be recreated at any moment if necessary.

:::{.callout-tip}
Remember: **Computation cost << Storage cost**
:::