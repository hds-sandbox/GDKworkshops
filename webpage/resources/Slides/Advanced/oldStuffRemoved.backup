# Your first pipeline

- An example pipeline
- parallelizing
- templates
- environments and containers
- resource usage

---

## Split and merge

A classical bioinformatics pipeline has splits and merge steps. We will work with this:

```{mermaid}
flowchart LR
  A0{data.fq} --> A["Split(data.fq)"]
  A --> B["table(part000.fq)"]
  A --> C["table(part002.fq)"]
  A --> D["table(part....fq)"]
  A --> E["table(part009.fq)"]
  B --> F["merge(table_[000-009].tsv)"]
  C --> F
  D --> F
  E --> F
  F --> G{table.tsv}
```

&nbsp;

where `split` and `table` (generate allele counts and statistics in a table) are done through `seqkit`.

---

## Software: gwf

There are many populare softwares for workflows (`Snakemake`, `Nextflow`), which requires learning their workflow language. We use instead `gwf`:

- Developed at AU (Dan at GenomeDK) and used also at MOMA, AUH, ...
  - Easy to find support
- In python, no need for new language
  - You can use all python functions to build your workflow!
- Easy to structure a workflow and check resource usage
  - Reusable templates
  - Very pedagogical
- Conda, Pixi, Container usage out-of-the-box

&nbsp;

Once you understand workflows with `gwf`, it is a matter of learning how to use other workflow languages, if you wish.

---

### Workflow objects in gwf

The whole workflow is contained in a `Workflow` object. This is initiated usually like this:

&nbsp;


```{.python}
#import libraries
from gwf import *

#initiate a workflow object
gwf = Workflow( )
```

&nbsp;

:::{.callout-note title="Using project resources"}
Now you have an empty workflow, which will use the few free resources each user has available. Resources from a project you are part of can be created using instead 

```{.python}
gwf = Workflow( defaults={"account": "PROJECT_NAME"} )
```
:::

---

### Templates in gwf

Now you can populate your `Workflow` object with `templates`, which are simple python functions. You can create *executors* for your templates (environments and containers, which will run the programs you need)! 

The **template is a blueprint for a specific block of the workflow**. 

For example, the 10 parallel table blocks can be described by one reusable template:

```{.python}
#create a software executor from conda environment `seqkitEnv`
from gwf.executors import Conda
conda_env = Conda("seqkitEnv")

def table(infile): #Name of the template

  inputs  = [infile]  #input file(s)
  outputs = [f"{infile}.fx2tab.tsv"] #name of the output(s)
  options = {"cores": 1, "memory": "1g", "walltime": "02:00:00"} #resources to use

  #the bash commands to run when using the template
  spec = f"""
  seqkit fx2tab -n -l -C A -C C -C G -C T -C N -g -G -s {infile} > {outputs[0]}
  """

  #here you provide all specifications defined in the template. 
  #This is usually always as below, 
  #apart from the executor, which you decide yourself
  return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec, executor=conda_env)
```

---

### Apply the template

The `split` and `table` templates are then applied to the relevant files:

```{.python}
parts=10 #how many parts to split into

# Apply the split template to data.fq to split in 10 parts. Call this step of the workflow "split"
# T1 collects info of the workflow step (e.g. output names)
T1 = gwf.target_from_template("split", split(infile="data.fq", parts=parts))

# Tabulate each part of the splitting and save the name of the tables in
# seqkit_output_files. We apply the template table to each input file of the
# for loop. The step of the workflow is called table_1, table_2, ..., table_10
seqkit_output_files = []
for i, infile in enumerate(T1.outputs):
   T2 = gwf.target_from_template(f"table_{i}", table(infile  = infile))
   seqkit_output_files.append(T2.outputs[0])
```

We try out exactly those template in the following exercise.

## Exercise I: workflow with conda containers

Prepare everything for the exercise: create a **new folder**, get **data and workflow file**

```{.bash}
 mkdir -p advancedGDK/pipeline
 cd advancedGDK/pipeline

 wget https://github.com/hds-sandbox/GDKworkshops/raw/refs/heads/main/Examples/smallGwf/data.fq -O data.fq
 wget https://github.com/hds-sandbox/GDKworkshops/raw/refs/heads/main/Examples/smallGwf/workflow.py -O workflow.py
```

---

Create a conda environment for `seqkit` and one for the `gwf` workflow software. Download the `seqkit` container as well.

&nbsp;

:::{.callout-tip}
If packages cannot be found, you need to add these default channels as well 

```{.bash}
conda config --add channels bioconda
conda config --add channels conda-forge
```
:::

&nbsp;

```{.bash}
conda config --add channels gwforg
conda create -y -n pipelineEnv gwf=2.1.1
#package for resource usage/check
conda install -y -n pipelineEnv -c micknudsen gwf-utilization
#env for seqkit
conda create -y -n seqkitEnv seqkit
#Container
singularity pull seqkit_2.10.0  https://depot.galaxyproject.org/singularity/seqkit:2.10.0--h9ee0642_0
```

---

Now look at the `status` of your workflow. You should recognize all the steps (*targets*). Those are marked `shouldrun`, because the outputs and/or inputs are not existent. Remember to activate the environment for `gwf`.

&nbsp;

```{.bash}
conda activate pipelineEnv

gwf -f workflow.py status
```

![](img/status0.png){fig-align="center" width=500px}

&nbsp;

:::{.callout-tip}
You do not need `-f workflow.py` if your workflow file has the name `workflow.py`, which is the default for `gwf`.
:::

---

Now, you might also want to look at how a specific *target* looks like when the workflow is built

```{.bash}
gwf info split
```

You will be able to see the actual inputs, outputs, and other targets it depends from/depending on it:

```{.python .scrollable  code-line-numbers="4-7|10|13-22|24|25|27-36"}
{
    "split": {
        "options": {
            "cores": 1,
            "memory": "4g",
            "walltime": "05:00:00"
        },
        "inputs": [
            "data.fq"
        ],
        "outputs": [
            "gwf_splitted/part001.fq",
            "gwf_splitted/part002.fq",
            "gwf_splitted/part003.fq",
            "gwf_splitted/part004.fq",
            "gwf_splitted/part005.fq",
            "gwf_splitted/part006.fq",
            "gwf_splitted/part007.fq",
            "gwf_splitted/part008.fq",
            "gwf_splitted/part009.fq",
            "gwf_splitted/part010.fq"
        ],
        "spec": "\n    seqkit split2 -O gwf_splitted --by-part 10 --by-part-prefix part data.fq\n    ",
        "dependencies": [],
        "dependents": [
            "table_6",
            "table_8",
            "table_3",
            "table_0",
            "table_1",
            "table_4",
            "table_5",
            "table_9",
            "table_7",
            "table_2"
        ]
    }
}
```

---

:::{.callout-tip}
Do you want to see the whole workflow in a text editor? Use the `less` viewer, 

```{.bash}
gwf info | less
```

or output into a text file:

```{.bash}
gwf info > workflow_explicit.txt
```
:::

&nbsp;

Now, you can run specific targets. Let's specify some names to test out our workflow. 

```{.bash}
gwf run split table_0
```

:::{.callout-tip}
You can run the entire workflow with `gwf run` when you are sure of your targets working correctly with the right resources.
:::

---

Check the status: the two turgets will be `submitted`, then `split` has to run first, and its *dependency* `table_0` will run when the file `part_001.fq` is generated! We use `watch` in front of the command to update its view every two seconds.

```{.bash}
watch gwf status
```

&nbsp;

At some point, you will see the `running` status (for a few seconds) and `completed` status.

![](img/gwfCompleted.png){fig-align="center" width=500px}


**Exercise break**

---

:::{.callout-tip}
When the status of a target is `failed`, or you want to verify messages on the command line, you can visualize the standard output and standard error messages of your target with the commands below (example with the `split` target):

&nbsp;

```{.bash}
gwf logs split 
gwf logs -e split
```
:::

---

### Resize the workflow resources and add a step

How many resources did `split` and `table_0` use? Run the utilization command:

```{.bash}
gwf utilization
```

![](img/gwfUtilization.png){fig-align="center" width=1000px}

&nbsp;

The table shows we underutilized the resources. Now open `workflow.py` and change your resource usage for the `split` and `table_` steps. Then, run the target:

```{.bash}
gwf run table_1
```
&nbsp;

Check again resource usage when the status is `completed`. Did it get better?

**Exercise break**

---

## Exercise II: singularity in workflows{.scrollable}

Now, you will change the executor for the template `table`. Your task is to:

  - open the `workflow.py` file
  - below importing the `Conda` module (line 2), add a new line with 

    `from gwf.executors import Singularity`
  - Now, define a new executor. Below the line where you define `conda_env = Conda("seqkitEnv")`, use a similar syntax for `Singularity`, where you provide the    container file as argument.
  - At the end of the `align` template, use the new executor instead of `conda_env`.

&nbsp;

Did you do it right? If yes, then you should be able to run the `combine` target:

```{.bash}
gwf run combine
```

and see its status become `completed` after some time, and all output files should be created in your folder! If not, something is wrong. Ask for help, or [look at the solution file, if you prefere](https://raw.githubusercontent.com/hds-sandbox/GDKworkshops/refs/heads/main/Examples/smallGwf/workflow_container.py).

:::{.callout-note}
Because `combine` depends on all `table_` targets, it will submit all those targets as well, which need to run first.
:::

**Exercise break**

---

## Exercise III: Your own target!{.scrollable}

Ok, now we want to extend the workflow and do quality control on the `part_###.fq` files. 

```{mermaid}
flowchart LR
  A0{data.fq} --> A["Split(data.fq)"]
  A --> B["table(part000.fq)"]
  A --> C["table(part002.fq)"]
  A --> D["table(part....fq)"]
  A --> E["table(part009.fq)"]
  B --> F["merge(table_[000-009].tsv)"]
  C --> F
  D --> F
  E --> F
  F --> G{table.tsv}
  B --> H["qc(part[000-009].fq)"]
  C --> H
  D --> H
  E --> H
  H --> I{"multiqc_report.html"}
```

---

You need to:

  - create a new environment called `qcEnv` where you install the two packages `fastqc multiqc`.
  - add a new executor called `qc_env` based on `Conda("qcEnv")`.
  - create a new target which starts with `def qc(data_folder)`
    - this will need all ten `gwf_splitted/part_###.fq` file as input files (you can copy the output file list of the `split` template, where you use a variable `{data_folder}` instead of the explicit folder name!)
    - as output you want a file called `[reports/multiqc_report.html]` (default name for the generated report)
    - as bash commands you need:
      ```{.bash}
      mkdir -p reports
      fastqc -o reports gdk_splitted/*.fq
      multiqc -o reports reports/
      ```
    - remember to set the correct executor at the end of the template
    - now you need to create one single target from the template, call it `qc`. You only need to give as input the name of the folder with the `fq` files


:::{.callout-tip}
- Before running any targets, always use `gwf info qc` to check dependencies.
- Copy previous similar templates and modify them where needed, instead of writing each template from scratch
:::

When you are sure you are done, then use `gwf run qc`. Its `status` should be `comlpeted` if it runs successfully.

Ask for help, or [look at the solution file, if you prefere](https://raw.githubusercontent.com/hds-sandbox/GDKworkshops/refs/heads/main/Examples/smallGwf/workflow_qc.py).

**Exercise break**

---

:::{.callout-note}
Good practices:

 - do not make tiny and numerous jobs, try to put together things with similar resource usage
 - start testing one of many parallel elements of a worlflow
    - determine resource usage (`gwf utilization` - needs a plugin, see earlier exercises)
    - adjust your templates' resources accordingly
 - verify often your code is correct
    - `gwf info target_name` to check dependencies
    - remember you are using python, check some of your code directly in it!
  - when done, you are reproducible if you share with your datasets:
    - container pull commands
    - conda environment(s) package list 
    - workflow files
:::



## Setup jupyter notebooks on VScode

You can do an easier setup of jupyter notebooks using VScode on your local computer. First of all, install VScode from [https://code.visualstudio.com/](https://code.visualstudio.com/) and the extension `Remote - SSH` from Microsoft and the `Jupyter` extension.